{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nxnZ7O7L2iFb"
      },
      "source": [
        "# Imports and Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tquh-SkP0yKA"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install datasets -q\n",
        "!pip install transformers -q\n",
        "!pip install seqeval -q\n",
        "!pip install ray[tune] -q\n",
        "!pip install numpyencoder -q\n",
        "!pip install wandb -q\n",
        "!pip install ray==2.2.0 -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SRn9Jxs0mzmS"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import itertools\n",
        "import pathlib\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import time\n",
        "from numpyencoder import NumpyEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import AutoTokenizer, TrainingArguments, Trainer\n",
        "from transformers import AutoModelForTokenClassification, DataCollatorForTokenClassification\n",
        "from datasets import load_metric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vIngDe3bARAg"
      },
      "outputs": [],
      "source": [
        "# Dataset functions\n",
        "def pandas2json(df, fname: str):\n",
        "    \"\"\"Convert pandas to json file\n",
        "    Args:\n",
        "        df (pd.DataFrame): Dataframe Object\n",
        "        fname (str): file name\n",
        "    \"\"\"\n",
        "\n",
        "    texts = []\n",
        "    for i in range(len(df)):\n",
        "        text_dict = {\n",
        "            \"text\": df['text'].iloc[i],\n",
        "            \"tags\": df['tags'].iloc[i]\n",
        "        }\n",
        "        texts.append(text_dict)\n",
        "\n",
        "    with open(fname, 'w', encoding='utf8') as file:\n",
        "        for text in texts:\n",
        "            json.dump(text, file, ensure_ascii=False)\n",
        "            file.write('\\n')\n",
        "\n",
        "\n",
        "def json2dict(fname: str, mode='r', encoding='utf8'):\n",
        "    \"\"\"Loads data from a json file into a dict object\n",
        "    \"\"\"\n",
        "    with open(fname, mode, encoding=encoding) as jfile:\n",
        "        data = json.load(jfile)\n",
        "\n",
        "    return data\n",
        "\n",
        "\n",
        "def dict2json(data: list, fname: str,\n",
        "                sort_keys=False, indent=None):\n",
        "    \"\"\"Saves the data in a json file\n",
        "    Args:\n",
        "        data (list[dict]): data in NM format:\n",
        "            {'text': str,\n",
        "            'entities': list[{'start': int, 'end': int, 'label': str, 'value': str}],\n",
        "            'anottation_status': str,\n",
        "            'notes': str}\n",
        "        fname (str): output file\n",
        "    \"\"\"\n",
        "\n",
        "    with open(fname, 'w', encoding='utf8') as file:\n",
        "        json.dump(data, file, ensure_ascii=False,\n",
        "                    sort_keys=sort_keys, indent=indent,\n",
        "                    cls=NumpyEncoder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xWtlXenHAX-X"
      },
      "outputs": [],
      "source": [
        "# Balancing functions\n",
        "def balance_datasets(d1: list, d2: list, upper_limit=0.75,\n",
        "                    balancing_range=0.2, names_list=None):\n",
        "    \"\"\"Balance NM NER dataset\n",
        "    Args:\n",
        "    d1, d2 (list[dict]): entities dict from __count_entities:\n",
        "    \"\"\"\n",
        "    # entities in each dataset\n",
        "    entities_d1, entities_d2 = (__count_entities(d1, names_list),\n",
        "                                __count_entities(d2, names_list))\n",
        "\n",
        "    __realizar_correcao(d1, d2, entities_d1, entities_d2,\n",
        "                        upper_limit=upper_limit,\n",
        "                        balancing_range=balancing_range)\n",
        "\n",
        "    __remove_null(d1); __remove_null(d2)\n",
        "\n",
        "\n",
        "def __count_entities(dataset, names_list=None):\n",
        "    \"\"\"Returns a entities dict in the format:\n",
        "    {\n",
        "        'names': list[str]\n",
        "        'ent_count': {'name': count (int)},     # dataset-wise\n",
        "        'doc_count': [{'name': count (int)}]    # element-wise\n",
        "        'pos': {'name': list[int]}\n",
        "    }\n",
        "    \"\"\"\n",
        "    names, ent_count, doc_count, pos = [], {}, [], {}\n",
        "    if  names_list:\n",
        "        for name in names_list:\n",
        "            names.append(name)\n",
        "            ent_count[name] = 0\n",
        "            pos[name] = []\n",
        "\n",
        "    for idx, doc in enumerate(dataset):\n",
        "        if doc is None: continue\n",
        "        doc_ent_count = {k: 0 for k in names}\n",
        "        for entity in doc['entities']:\n",
        "            ent_name = entity['label']\n",
        "            if ent_name not in names:\n",
        "                names.append(ent_name)\n",
        "                ent_count[ent_name] = 0\n",
        "                pos[ent_name] = []\n",
        "                doc_ent_count[ent_name] = 0\n",
        "                for doc in doc_count: doc.update({ent_name: 0})\n",
        "\n",
        "            ent_count[ent_name] += 1\n",
        "            pos[ent_name].append(idx)\n",
        "            doc_ent_count[ent_name] += 1\n",
        "\n",
        "        doc_count.append(doc_ent_count)\n",
        "\n",
        "    return {'names': names, 'ent_count': ent_count,\n",
        "            'doc_count': doc_count, 'pos': pos}\n",
        "\n",
        "\n",
        "def __transfer_entity(destination, source, idx):\n",
        "    destination.append(source[idx])\n",
        "    source[idx] = None\n",
        "\n",
        "\n",
        "def __balance_entity(destination, source, entities_dest, entities_src,\n",
        "                    qtd, entity):\n",
        "    \"\"\"Transfere 'qtd' documentos que contém uma entidade\n",
        "    do dataset de origem (source) para o dataset de destino (destination).\n",
        "    \"\"\"\n",
        "\n",
        "    qtd = abs(qtd)\n",
        "    while qtd > 0:\n",
        "        for idx, doc in enumerate(entities_src['doc_count']):\n",
        "            if doc[entity]:\n",
        "                qtd -= doc[entity]\n",
        "                __transfer_entity(destination, source, idx)\n",
        "\n",
        "                for entity_name in doc.keys():\n",
        "                    entities_src['ent_count'][entity_name] -= doc[entity_name]\n",
        "                    entities_dest['ent_count'][entity_name] += doc[entity_name]\n",
        "                    doc[entity_name] = 0\n",
        "                break\n",
        "\n",
        "\n",
        "def __realizar_correcao(d1, d2, entities_d1, entities_d2, upper_limit=0.75,\n",
        "                        balancing_range=0.10):\n",
        "    for entity in entities_d1['names']:\n",
        "        e1, e2 = entities_d1['ent_count'][entity], entities_d2['ent_count'][entity]\n",
        "        percent = e1/(e1+e2)\n",
        "        unit_percent = 1/(e1+e2)\n",
        "\n",
        "        # destination = d2, source = d1\n",
        "        if percent > upper_limit:\n",
        "            qtd = (percent - upper_limit + balancing_range/2) / unit_percent\n",
        "            __balance_entity(d2, d1, entities_d2, entities_d1, round(qtd), entity)\n",
        "\n",
        "        # destination = d1, source = d2\n",
        "        if percent < upper_limit - balancing_range:\n",
        "            qtd = (upper_limit - percent - balancing_range/2) / unit_percent\n",
        "            __balance_entity(d1, d2, entities_d1, entities_d2, round(qtd), entity)\n",
        "\n",
        "\n",
        "def __remove_null(dataset):\n",
        "    for doc in reversed(dataset):\n",
        "        if doc is None:\n",
        "            dataset.remove(doc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c6TwqEHL4dnG"
      },
      "outputs": [],
      "source": [
        "# Stats functions\n",
        "def get_entities_percentage(entities_d1, entities_d2, print_results=True):\n",
        "    percents = [e1/(e1+e2)\n",
        "    for e1, e2 in zip(entities_d1['ent_count'].values(), entities_d2['ent_count'].values())\n",
        "    ]\n",
        "\n",
        "    text = ''\n",
        "    for percent, entity in zip(percents, entities_d1['names']):\n",
        "        text += f'{percent}\\t{entity}\\n'\n",
        "\n",
        "    if print_results:\n",
        "        print(text, end='')\n",
        "\n",
        "    return text\n",
        "\n",
        "\n",
        "def get_entities_count(entities_d1, print_results=True):\n",
        "    text = ''\n",
        "    for count, entity in zip(entities_d1['ent_count'].values(), entities_d1['names']):\n",
        "        text += f'{count} \\t{entity}\\n'\n",
        "\n",
        "    if print_results:\n",
        "        print(text, end='')\n",
        "\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Zz6TS4FNtJM"
      },
      "outputs": [],
      "source": [
        "# Tokenizing functions\n",
        "def get_ent_label(entity_name: str) -> int:\n",
        "    label_n = 0\n",
        "    if entity_name=='CABECALHO':\n",
        "        label_n=1\n",
        "    elif entity_name=='SUBCABECALHO':\n",
        "        label_n=3\n",
        "    else:\n",
        "        label_n=5\n",
        "    return label_n\n",
        "\n",
        "\n",
        "def create_label_vector(doc, input_ids, tokenizer):\n",
        "    vetor=np.zeros(512)\n",
        "    for ent_dict in doc['entities']:\n",
        "        ent_label = get_ent_label(ent_dict['label'])\n",
        "        entidade = doc['text'][ent_dict['start'] : ent_dict['end']]\n",
        "        tokenized_entity = tokenizer(entidade, is_split_into_words=False)\n",
        "\n",
        "        for token_idx, input_id in enumerate(input_ids):\n",
        "            entity_ids = tokenized_entity['input_ids']\n",
        "            if entity_ids[1] == input_id:\n",
        "                if entity_ids[1:-1] == input_ids[token_idx : token_idx+(len(entity_ids)-2)]:\n",
        "                    vetor[token_idx] = ent_label\n",
        "                    vetor[token_idx+1:token_idx+(len(entity_ids)-2)] = ent_label+1\n",
        "                    break\n",
        "\n",
        "    for idx, id in enumerate(input_ids):\n",
        "        if id == 101 or id ==102:\n",
        "            vetor[idx] = -100\n",
        "\n",
        "    return vetor.tolist()\n",
        "\n",
        "\n",
        "def tokenize_dataset(dataset, tokenizer, stride=0):\n",
        "    tokenized_dataset = []\n",
        "    for doc in dataset:\n",
        "        tokenized_text = tokenizer(doc['text'], padding='max_length', truncation=True,\n",
        "                                    stride = stride,\n",
        "                                    max_length=512, is_split_into_words=False,\n",
        "                                    return_overflowing_tokens=True,)\n",
        "\n",
        "        for idx, _ in enumerate(tokenized_text['overflow_to_sample_mapping']):\n",
        "            new_doc = {\n",
        "                'input_ids': tokenized_text.input_ids[idx],\n",
        "                'attention_mask': tokenized_text.attention_mask[idx],\n",
        "                'labels': create_label_vector(doc, tokenized_text.input_ids[idx], tokenizer),\n",
        "            }\n",
        "            tokenized_dataset.append(new_doc)\n",
        "\n",
        "    return tokenized_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pls0Espw9DgE"
      },
      "outputs": [],
      "source": [
        "# Training class\n",
        "class NM_Trainer():\n",
        "    \"\"\"Trainer for NM dataset.\n",
        "    Expects the train and test datasets to already be tokenized and balanced.\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                treino: dict,\n",
        "                teste: dict,\n",
        "                label_names: str,\n",
        "                metric,\n",
        "                entities_names: str = None,\n",
        "                tokenizer = None,\n",
        "                use_wandb = False,\n",
        "                wandb_run_name = None,\n",
        "                ) -> None:\n",
        "        self.treino = treino\n",
        "        self.teste = teste\n",
        "        self.metric = metric\n",
        "        self.label_names = label_names\n",
        "        self.entities_names = entities_names\n",
        "        self.tokenizer = tokenizer\n",
        "        if tokenizer is None:\n",
        "            self.tokenizer = AutoTokenizer.from_pretrained('neuralmind/bert-base-portuguese-cased',\n",
        "                                                            do_lower_case=False)\n",
        "        self.trainer = self.__get_trainer()\n",
        "        if use_wandb:\n",
        "            setattr(self.trainer.args, \"report_to\", \"wandb\")\n",
        "            self.__set_wandb_run_name(wandb_run_name)\n",
        "\n",
        "    def train(self):\n",
        "        return self.trainer.train()\n",
        "\n",
        "    def return_metrics(self) -> dict:\n",
        "        predictions, labels, _ = self.trainer.predict(self.teste)\n",
        "        predictions = np.argmax(predictions, axis=2)\n",
        "\n",
        "        # Remove ignored index (special tokens)\n",
        "        true_predictions = [\n",
        "            [self.label_names[p] for (p, l) in zip(prediction, label) if l != -100]\n",
        "            for prediction, label in zip(predictions, labels)\n",
        "        ]\n",
        "        true_labels = [\n",
        "            [self.label_names[l] for (p, l) in zip(prediction, label) if l != -100]\n",
        "            for prediction, label in zip(predictions, labels)\n",
        "        ]\n",
        "\n",
        "        return self.metric.compute(predictions=true_predictions, references=true_labels)\n",
        "\n",
        "    def __get_trainer(self):\n",
        "        def model_init():\n",
        "            return AutoModelForTokenClassification.from_pretrained(\"neuralmind/bert-base-portuguese-cased\", num_labels=7)\n",
        "\n",
        "        def compute_metrics(p):\n",
        "            predictions, labels = p\n",
        "            predictions = np.argmax(predictions, axis=2)\n",
        "\n",
        "            # Remove ignored index (special tokens)\n",
        "            true_predictions = [\n",
        "                [self.label_names[p] for (p, l) in zip(prediction, label) if l != -100]\n",
        "                for prediction, label in zip(predictions, labels)\n",
        "            ]\n",
        "            true_labels = [\n",
        "                [self.label_names[l] for (p, l) in zip(prediction, label) if l != -100]\n",
        "                for prediction, label in zip(predictions, labels)\n",
        "            ]\n",
        "\n",
        "            results = self.metric.compute(predictions=true_predictions, references=true_labels)\n",
        "\n",
        "            return {\n",
        "                    \"precision\": results[\"overall_precision\"],\n",
        "                    \"recall\": results[\"overall_recall\"],\n",
        "                    \"f1\": results[\"overall_f1\"],\n",
        "                    #\"accuracy\": results[\"overall_accuracy\"],\n",
        "                    }\n",
        "\n",
        "        data_collator = DataCollatorForTokenClassification(self.tokenizer)\n",
        "        hyperparameters={\n",
        "            'learning_rate': 4.076831342095183e-05,\n",
        "            'num_train_epochs': 3,\n",
        "            'per_device_train_batch_size': 4\n",
        "        }\n",
        "        batch_size = hyperparameters['per_device_train_batch_size']\n",
        "        logging_steps = len(self.treino) // batch_size\n",
        "        epochs = hyperparameters['num_train_epochs']\n",
        "        training_args = TrainingArguments(\n",
        "            output_dir = \"results\",\n",
        "            num_train_epochs = epochs,\n",
        "            per_device_train_batch_size = batch_size,\n",
        "            per_device_eval_batch_size = batch_size,\n",
        "            evaluation_strategy = \"epoch\",\n",
        "            metric_for_best_model = \"f1\",\n",
        "            disable_tqdm = False,\n",
        "            logging_steps = logging_steps,\n",
        "            gradient_accumulation_steps = 2,\n",
        "            eval_accumulation_steps = 2,\n",
        "            learning_rate = hyperparameters['learning_rate'],\n",
        "        )\n",
        "        trainer = Trainer(\n",
        "            model_init=model_init,\n",
        "            args=training_args,\n",
        "            train_dataset=self.treino,\n",
        "            eval_dataset=self.teste,\n",
        "            data_collator=data_collator,\n",
        "            tokenizer=self.tokenizer,\n",
        "            compute_metrics=compute_metrics\n",
        "        )\n",
        "\n",
        "        return trainer\n",
        "\n",
        "\n",
        "    def __set_wandb_run_name(self, run_name: str):\n",
        "        if run_name is None:\n",
        "            run_name = \"huggingface\"\n",
        "\n",
        "        setattr(self.trainer.args, \"run_name\", run_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Em4cY0SnWYj1"
      },
      "outputs": [],
      "source": [
        "# Pipeline\n",
        "def get_trainer(\n",
        "                dataset: dict,\n",
        "                label_names,\n",
        "                metric,\n",
        "                balance=True,\n",
        "                stride=0,\n",
        "                tokenizer=None,\n",
        "                test_size=0.2,\n",
        "                random_state=42,\n",
        "                balancing_upper_limit=0.75,\n",
        "                balancing_range=0.20,\n",
        "                entities_names=None,\n",
        "                use_wandb = False,\n",
        "                wandb_run_name = None,\n",
        "                ):\n",
        "    #dataset\n",
        "    treino, teste = train_test_split(dataset,\n",
        "                                    test_size=test_size,\n",
        "                                    random_state=random_state)\n",
        "\n",
        "    #balanceamento\n",
        "    if balance:\n",
        "        balance_datasets(treino, teste,\n",
        "                        upper_limit=balancing_upper_limit,\n",
        "                        balancing_range=balancing_range,\n",
        "                        names_list=entities_names)\n",
        "\n",
        "    #tokenização\n",
        "    if not tokenizer:\n",
        "        tokenizer = AutoTokenizer.from_pretrained('neuralmind/bert-base-portuguese-cased',\n",
        "                                                    do_lower_case=False)\n",
        "    treino = tokenize_dataset(treino, tokenizer,\n",
        "                                stride=stride)\n",
        "    teste = tokenize_dataset(teste, tokenizer,\n",
        "                                stride=stride)\n",
        "\n",
        "    trainer = NM_Trainer(treino, teste,\n",
        "                        label_names=label_names,\n",
        "                        metric=metric,\n",
        "                        tokenizer=tokenizer,\n",
        "                        use_wandb=use_wandb,\n",
        "                        wandb_run_name=wandb_run_name,)\n",
        "\n",
        "    return trainer\n",
        "\n",
        "def run_test(\n",
        "        dataset: dict,\n",
        "        label_names,\n",
        "        metric,\n",
        "        balance=True,\n",
        "        stride=0,\n",
        "        tokenizer=None,\n",
        "        test_size=0.2,\n",
        "        random_state=42,\n",
        "        balancing_upper_limit=0.75,\n",
        "        balancing_range=0.20,\n",
        "        entities_names=None,\n",
        "        use_wandb = False,\n",
        "        wandb_run_name = None,\n",
        "        ):\n",
        "    trainer = get_trainer(dataset=dataset,\n",
        "                        label_names=label_names,\n",
        "                        metric=metric,\n",
        "                        balance=balance,\n",
        "                        stride=stride,\n",
        "                        tokenizer=tokenizer,\n",
        "                        test_size=test_size,\n",
        "                        random_state=random_state,\n",
        "                        balancing_upper_limit=balancing_upper_limit,\n",
        "                        balancing_range=balancing_range,\n",
        "                        entities_names=entities_names,\n",
        "                        use_wandb = use_wandb,\n",
        "                        wandb_run_name = wandb_run_name,)\n",
        "\n",
        "    trainer.train()\n",
        "\n",
        "    return trainer.return_metrics()\n",
        "\n",
        "\n",
        "def test_with_checkpoints(params_list,\n",
        "                        output_name,\n",
        "                        dataset: dict,\n",
        "                        label_names,\n",
        "                        metric,\n",
        "                        entities_names=None,\n",
        "                        output_dir='checkpoints/',\n",
        "                        step=0.1,\n",
        "                        use_wandb=False,\n",
        "                        wandb_config=None,\n",
        "                        ):\n",
        "    step = round(len(params_list)*step)\n",
        "    checkpoints = [x for x in range(step, len(params_list)-step, step)]\n",
        "    test_results = {}\n",
        "    run = 0\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "    for parameters in params_list:\n",
        "        timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n",
        "        run += 1\n",
        "        wandb_run_name = f\"{timestr}_{run}_\" + \"_\".join([f'{k}-{v}' for k,v in parameters.items()])\n",
        "\n",
        "        if use_wandb:\n",
        "            wandb_run = wandb.init(reinit=True, name=wandb_run_name,\n",
        "                                    config=wandb_config)\n",
        "\n",
        "        result = run_test(\n",
        "            dataset=dataset,\n",
        "            label_names=label_names,\n",
        "            metric=metric,\n",
        "            entities_names=entities_names,\n",
        "            use_wandb=use_wandb,\n",
        "            wandb_run_name=wandb_run_name,\n",
        "            **parameters\n",
        "        )\n",
        "\n",
        "        if use_wandb:\n",
        "            wandb_run.finish()\n",
        "\n",
        "        test_results[f'run{run}'] = {\n",
        "            'parameters': parameters,\n",
        "            'result': result,\n",
        "        }\n",
        "        if run in checkpoints:\n",
        "            fname = f\"{output_dir}{output_name}_{timestr}_run{run}.json\"\n",
        "            dict2json(test_results, fname, sort_keys=False, indent=2)\n",
        "\n",
        "    timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n",
        "    dict2json(test_results, f\"{output_dir}{output_name}_{timestr}_final.json\",\n",
        "                sort_keys=False, indent=2)\n",
        "\n",
        "    return test_results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EZvE8a2D2lWu"
      },
      "source": [
        "# Dataset and globals"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mf2ww9vFWOTJ"
      },
      "outputs": [],
      "source": [
        "# globals\n",
        "current_dir = str(pathlib.Path(__file__).parent.resolve()) + \"/\"\n",
        "data_path = current_dir + \"NM_dataset.json\"\n",
        "dataset = [doc for doc in json2dict(data_path)]\n",
        "tokenizer = AutoTokenizer.from_pretrained('neuralmind/bert-base-portuguese-cased', do_lower_case=False)\n",
        "metric = load_metric(\"seqeval\")\n",
        "entities_names = ['COMECO RECORTE', 'CABECALHO', 'SUBCABECALHO']\n",
        "label_names={\n",
        "    0: 'O',\n",
        "    1: 'B-CABECALHO',\n",
        "    2: 'I-CABECALHO',\n",
        "    3: 'B-SUBCABECALHO',\n",
        "    4: 'I-SUBCABECALHO',\n",
        "    5: 'B-COMECO_RECORTE',\n",
        "    6: 'I-COMECO_RECORTE',\n",
        "}\n",
        "wandb_config = {\n",
        "    \"project\": \"SWNM\",\n",
        "    \"entity\": \"chinagab\",\n",
        "    \"api_key\": \"7d7deda5ab99137996e34e47dc688b1d6b4d179c\",\n",
        "    \"log_config\": True\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V7oWF_HHFyVc"
      },
      "source": [
        "# Running tests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "45E25WYLF0it"
      },
      "outputs": [],
      "source": [
        "test_hyperparameters = {\n",
        "    #\"balance\": [True, False],\n",
        "    \"balancing_range\": [round(i,2) for i in np.arange(0.3, 0.4, 0.05)],\n",
        "    \"balancing_upper_limit\": [round(i,2) for i in np.arange(0.65, 0.9, 0.1)],\n",
        "    \"test_size\": [round(i,2) for i in np.arange(0.1, 0.4, 0.05)],\n",
        "    \"stride\": [0, 128, 256],\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o9oMi6rJF0ix",
        "outputId": "4021d001-80fe-4ab1-c362-9494acf8c38f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "balancing_range 3 3 [0.3, 0.35, 0.4]\n",
            "balancing_upper_limit 3 9 [0.65, 0.75, 0.85]\n",
            "test_size 7 63 [0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4]\n",
            "stride 3 189 [0, 128, 256]\n",
            "\n",
            "Number of tests: 189\n"
          ]
        }
      ],
      "source": [
        "params_used = [k for k in test_hyperparameters]\n",
        "params_list = list(itertools.product(*(test_hyperparameters.values())))\n",
        "params_list = [{k:v for k,v in zip(params_used, p)} for p in params_list]\n",
        "a = 1\n",
        "for k,v in test_hyperparameters.items():\n",
        "  a *= len(v)\n",
        "  print(k, len(v), a, v)\n",
        "  \n",
        "print('\\nNumber of tests:', a)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u-AEyoqqSwnO"
      },
      "outputs": [],
      "source": [
        "for idx, par in enumerate(params_list):\n",
        "    print(idx, par)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IVfItlfR5aKv"
      },
      "outputs": [],
      "source": [
        "test_with_checkpoints(params_list=params_list,\n",
        "                        output_name='SWNM-dataset',\n",
        "                        dataset=dataset,\n",
        "                        label_names=label_names,\n",
        "                        metric=metric,\n",
        "                        entities_names=entities_names,\n",
        "                        output_dir=current_dir+\"checkpoints/\",\n",
        "                        step=0.05,\n",
        "                        use_wandb=True,\n",
        "                        wandb_config=wandb_config,)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.2"
    },
    "vscode": {
      "interpreter": {
        "hash": "26de051ba29f2982a8de78e945f0abaf191376122a1563185a90213a26c5da77"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
