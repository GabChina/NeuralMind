{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Imports and Functions"
      ],
      "metadata": {
        "id": "nFbqS-cI9sBl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ijkM99z3vLr6"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B7RcoK3CW-LR"
      },
      "outputs": [],
      "source": [
        "CUDA_LAUNCH_BLOCKING=1"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "metadata": {
        "id": "OIyQaaBG0diB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tquh-SkP0yKA"
      },
      "outputs": [],
      "source": [
        "!pip install datasets -q\n",
        "!pip install tokenizers -q\n",
        "!pip install transformers -q\n",
        "!pip install seqeval -q\n",
        "#!pip install optuna -q\n",
        "!pip install -Uq ray[tune] wandb"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import itertools\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import time\n",
        "from numpyencoder import NumpyEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import AutoTokenizer, TrainingArguments, Trainer\n",
        "from transformers import AutoModelForTokenClassification, DataCollatorForTokenClassification\n",
        "from datasets import load_metric\n",
        "from ray.tune.schedulers import PopulationBasedTraining\n",
        "from ray.tune.logger import DEFAULT_LOGGERS\n",
        "from ray import tune\n",
        "from ray.tune.integration.wandb import WandbLoggerCallback\n",
        "import wandb\n",
        "from __future__ import annotations"
      ],
      "metadata": {
        "id": "tmEKe6E_-H12"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset functions\n",
        "def pandas2json(df, fname: str):\n",
        "    \"\"\"Convert pandas to json file\n",
        "    Args:\n",
        "        df (pd.DataFrame): Dataframe Object\n",
        "        fname (str): file name\n",
        "    \"\"\"\n",
        "\n",
        "    texts = []\n",
        "    for i in range(len(df)):\n",
        "        text_dict = {\n",
        "            \"text\": df['text'].iloc[i],\n",
        "            \"tags\": df['tags'].iloc[i]\n",
        "        }\n",
        "        texts.append(text_dict)\n",
        "\n",
        "    with open(fname, 'w', encoding='utf8') as file:\n",
        "        for text in texts:\n",
        "            json.dump(text, file, ensure_ascii=False)\n",
        "            file.write('\\n')\n",
        "\n",
        "\n",
        "def json2dict(fname: str, mode='r', encoding='utf8'):\n",
        "    \"\"\"Loads data from a json file into a dict object\n",
        "    \"\"\"\n",
        "    with open(fname, mode, encoding=encoding) as jfile:\n",
        "        data = json.load(jfile)\n",
        "\n",
        "    return data\n",
        "\n",
        "\n",
        "def dict2json(data: list, fname: str,\n",
        "                sort_keys=False, indent=None):\n",
        "    \"\"\"Saves the data in a json file\n",
        "    Args:\n",
        "        data (list[dict]): data in NM format:\n",
        "            {'text': str,\n",
        "            'entities': list[{'start': int, 'end': int, 'label': str, 'value': str}],\n",
        "            'anottation_status': str,\n",
        "            'notes': str}\n",
        "        fname (str): output file\n",
        "    \"\"\"\n",
        "\n",
        "    with open(fname, 'w', encoding='utf8') as file:\n",
        "        json.dump(data, file, ensure_ascii=False,\n",
        "                    sort_keys=sort_keys, indent=indent,\n",
        "                    cls=NumpyEncoder)"
      ],
      "metadata": {
        "id": "9Bo7o20b9za7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Balancing functions\n",
        "def balance_datasets(d1: list, d2: list, upper_limit=0.75,\n",
        "                    balancing_range=0.2, names_list=None):\n",
        "    \"\"\"Balance NM NER dataset\n",
        "    Args:\n",
        "    d1, d2 (list[dict]): entities dict from __count_entities:\n",
        "    \"\"\"\n",
        "    # entities in each dataset\n",
        "    entities_d1, entities_d2 = (__count_entities(d1, names_list),\n",
        "                                __count_entities(d2, names_list))\n",
        "\n",
        "    __realizar_correcao(d1, d2, entities_d1, entities_d2,\n",
        "                        upper_limit=upper_limit,\n",
        "                        balancing_range=balancing_range)\n",
        "\n",
        "    __remove_null(d1); __remove_null(d2)\n",
        "\n",
        "\n",
        "def __count_entities(dataset, names_list=None):\n",
        "    \"\"\"Returns a entities dict in the format:\n",
        "    {\n",
        "        'names': list[str]\n",
        "        'ent_count': {'name': count (int)},     # dataset-wise\n",
        "        'doc_count': [{'name': count (int)}]    # element-wise\n",
        "        'pos': {'name': list[int]}\n",
        "    }\n",
        "    \"\"\"\n",
        "    names, ent_count, doc_count, pos = [], {}, [], {}\n",
        "    if  names_list:\n",
        "        for name in names_list:\n",
        "            names.append(name)\n",
        "            ent_count[name] = 0\n",
        "            pos[name] = []\n",
        "\n",
        "    for idx, doc in enumerate(dataset):\n",
        "        if doc is None: continue\n",
        "        doc_ent_count = {k: 0 for k in names}\n",
        "        for entity in doc['entities']:\n",
        "            ent_name = entity['label']\n",
        "            if ent_name not in names:\n",
        "                names.append(ent_name)\n",
        "                ent_count[ent_name] = 0\n",
        "                pos[ent_name] = []\n",
        "                doc_ent_count[ent_name] = 0\n",
        "                for doc in doc_count: doc.update({ent_name: 0})\n",
        "\n",
        "            ent_count[ent_name] += 1\n",
        "            pos[ent_name].append(idx)\n",
        "            doc_ent_count[ent_name] += 1\n",
        "\n",
        "        doc_count.append(doc_ent_count)\n",
        "\n",
        "    return {'names': names, 'ent_count': ent_count,\n",
        "            'doc_count': doc_count, 'pos': pos}\n",
        "\n",
        "\n",
        "def __transfer_entity(destination, source, idx):\n",
        "    destination.append(source[idx])\n",
        "    source[idx] = None\n",
        "\n",
        "\n",
        "def __balance_entity(destination, source, entities_dest, entities_src,\n",
        "                    qtd, entity):\n",
        "    \"\"\"Transfere 'qtd' documentos que contÃ©m uma entidade\n",
        "    do dataset de origem (source) para o dataset de destino (destination).\n",
        "    \"\"\"\n",
        "\n",
        "    qtd = abs(qtd)\n",
        "    while qtd > 0:\n",
        "        for idx, doc in enumerate(entities_src['doc_count']):\n",
        "            if doc[entity]:\n",
        "                qtd -= doc[entity]\n",
        "                __transfer_entity(destination, source, idx)\n",
        "\n",
        "                for entity_name in doc.keys():\n",
        "                    entities_src['ent_count'][entity_name] -= doc[entity_name]\n",
        "                    entities_dest['ent_count'][entity_name] += doc[entity_name]\n",
        "                    doc[entity_name] = 0\n",
        "                break\n",
        "\n",
        "\n",
        "def __realizar_correcao(d1, d2, entities_d1, entities_d2, upper_limit=0.75,\n",
        "                        balancing_range=0.10):\n",
        "    for entity in entities_d1['names']:\n",
        "        e1, e2 = entities_d1['ent_count'][entity], entities_d2['ent_count'][entity]\n",
        "        percent = e1/(e1+e2)\n",
        "        unit_percent = 1/(e1+e2)\n",
        "\n",
        "        # destination = d2, source = d1\n",
        "        if percent > upper_limit:\n",
        "            qtd = (percent - upper_limit + balancing_range/2) / unit_percent\n",
        "            __balance_entity(d2, d1, entities_d2, entities_d1, round(qtd), entity)\n",
        "\n",
        "        # destination = d1, source = d2\n",
        "        if percent < upper_limit - balancing_range:\n",
        "            qtd = (upper_limit - percent - balancing_range/2) / unit_percent\n",
        "            __balance_entity(d1, d2, entities_d1, entities_d2, round(qtd), entity)\n",
        "\n",
        "\n",
        "def __remove_null(dataset):\n",
        "    for doc in reversed(dataset):\n",
        "        if doc is None:\n",
        "            dataset.remove(doc)"
      ],
      "metadata": {
        "id": "jt0TN2Kb-1JC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Stats functions\n",
        "def get_entities_percentage(entities_d1, entities_d2, print_results=True):\n",
        "    percents = [e1/(e1+e2)\n",
        "    for e1, e2 in zip(entities_d1['ent_count'].values(), entities_d2['ent_count'].values())\n",
        "    ]\n",
        "\n",
        "    text = ''\n",
        "    for percent, entity in zip(percents, entities_d1['names']):\n",
        "        text += f'{percent}\\t{entity}\\n'\n",
        "\n",
        "    if print_results:\n",
        "        print(text, end='')\n",
        "\n",
        "    return text\n",
        "\n",
        "\n",
        "def get_entities_count(entities_d1, print_results=True):\n",
        "    text = ''\n",
        "    for count, entity in zip(entities_d1['ent_count'].values(), entities_d1['names']):\n",
        "        text += f'{count} \\t{entity}\\n'\n",
        "\n",
        "    if print_results:\n",
        "        print(text, end='')\n",
        "\n",
        "    return text"
      ],
      "metadata": {
        "id": "juBiSBkZ_BI1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenizing functions\n",
        "def get_ent_label(entity_name: str) -> int:\n",
        "    label_n = 0\n",
        "    if entity_name=='CABECALHO':\n",
        "        label_n=1\n",
        "    elif entity_name=='SUBCABECALHO':\n",
        "        label_n=3\n",
        "    else:\n",
        "        label_n=5\n",
        "    return label_n\n",
        "\n",
        "\n",
        "def create_label_vector(doc, input_ids, tokenizer):\n",
        "    vetor=np.zeros(512)\n",
        "    for ent_dict in doc['entities']:\n",
        "        ent_label = get_ent_label(ent_dict['label'])\n",
        "        entidade = doc['text'][ent_dict['start'] : ent_dict['end']]\n",
        "        tokenized_entity = tokenizer(entidade, is_split_into_words=False)\n",
        "\n",
        "        for token_idx, input_id in enumerate(input_ids):\n",
        "            entity_ids = tokenized_entity['input_ids']\n",
        "            if entity_ids[1] == input_id:\n",
        "                if entity_ids[1:-1] == input_ids[token_idx : token_idx+(len(entity_ids)-2)]:\n",
        "                    vetor[token_idx] = ent_label\n",
        "                    vetor[token_idx+1:token_idx+(len(entity_ids)-2)] = ent_label+1\n",
        "                    break\n",
        "\n",
        "    for idx, id in enumerate(input_ids):\n",
        "        if id == 101 or id ==102:\n",
        "            vetor[idx] = -100\n",
        "\n",
        "    return vetor.tolist()\n",
        "\n",
        "\n",
        "def tokenize_dataset(dataset, tokenizer, stride=0):\n",
        "    tokenized_dataset = []\n",
        "    for doc in dataset:\n",
        "        tokenized_text = tokenizer(doc['text'], padding='max_length', truncation=True,\n",
        "                                    stride = stride,\n",
        "                                    max_length=512, is_split_into_words=False,\n",
        "                                    return_overflowing_tokens=True,)\n",
        "\n",
        "        for idx, _ in enumerate(tokenized_text['overflow_to_sample_mapping']):\n",
        "            new_doc = {\n",
        "                'input_ids': tokenized_text.input_ids[idx],\n",
        "                'attention_mask': tokenized_text.attention_mask[idx],\n",
        "                'labels': create_label_vector(doc, tokenized_text.input_ids[idx], tokenizer),\n",
        "            }\n",
        "            tokenized_dataset.append(new_doc)\n",
        "\n",
        "    return tokenized_dataset"
      ],
      "metadata": {
        "id": "AzaGnIej_GDe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training class\n",
        "class NM_Trainer():\n",
        "    \"\"\"Trainer for NM dataset.\n",
        "    Expects the train and test datasets to already be tokenized and balanced.\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                treino: dict,\n",
        "                teste: dict,\n",
        "                label_names: str,\n",
        "                metric,\n",
        "                entities_names: str = None,\n",
        "                tokenizer = None,\n",
        "                use_wandb = False,\n",
        "                wandb_run_name = None,\n",
        "                ) -> None:\n",
        "        self.treino = treino\n",
        "        self.teste = teste\n",
        "        self.metric = metric\n",
        "        self.label_names = label_names\n",
        "        self.entities_names = entities_names\n",
        "        self.tokenizer = tokenizer\n",
        "        if tokenizer is None:\n",
        "            self.tokenizer = AutoTokenizer.from_pretrained('neuralmind/bert-base-portuguese-cased',\n",
        "                                                            do_lower_case=False)\n",
        "        self.trainer = self.__get_trainer()\n",
        "        if use_wandb:\n",
        "            setattr(self.trainer.args, \"report_to\", \"wandb\")\n",
        "            self.__set_wandb_run_name(wandb_run_name)\n",
        "\n",
        "    def train(self):\n",
        "        return self.trainer.train()\n",
        "\n",
        "    def return_metrics(self) -> dict:\n",
        "        predictions, labels, _ = self.trainer.predict(self.teste)\n",
        "        predictions = np.argmax(predictions, axis=2)\n",
        "\n",
        "        # Remove ignored index (special tokens)\n",
        "        true_predictions = [\n",
        "            [self.label_names[p] for (p, l) in zip(prediction, label) if l != -100]\n",
        "            for prediction, label in zip(predictions, labels)\n",
        "        ]\n",
        "        true_labels = [\n",
        "            [self.label_names[l] for (p, l) in zip(prediction, label) if l != -100]\n",
        "            for prediction, label in zip(predictions, labels)\n",
        "        ]\n",
        "\n",
        "        return self.metric.compute(predictions=true_predictions, references=true_labels)\n",
        "\n",
        "    def __get_trainer(self):\n",
        "        def model_init():\n",
        "            return AutoModelForTokenClassification.from_pretrained(\"neuralmind/bert-base-portuguese-cased\", num_labels=7)\n",
        "\n",
        "        def compute_metrics(p):\n",
        "            predictions, labels = p\n",
        "            predictions = np.argmax(predictions, axis=2)\n",
        "\n",
        "            # Remove ignored index (special tokens)\n",
        "            true_predictions = [\n",
        "                [self.label_names[p] for (p, l) in zip(prediction, label) if l != -100]\n",
        "                for prediction, label in zip(predictions, labels)\n",
        "            ]\n",
        "            true_labels = [\n",
        "                [self.label_names[l] for (p, l) in zip(prediction, label) if l != -100]\n",
        "                for prediction, label in zip(predictions, labels)\n",
        "            ]\n",
        "\n",
        "            results = self.metric.compute(predictions=true_predictions, references=true_labels)\n",
        "\n",
        "            return {\n",
        "                    \"precision\": results[\"overall_precision\"],\n",
        "                    \"recall\": results[\"overall_recall\"],\n",
        "                    \"f1\": results[\"overall_f1\"],\n",
        "                    #\"accuracy\": results[\"overall_accuracy\"],\n",
        "                    }\n",
        "\n",
        "        data_collator = DataCollatorForTokenClassification(self.tokenizer)\n",
        "        hyperparameters={\n",
        "            'learning_rate': 4.076831342095183e-05,\n",
        "            'num_train_epochs': 3,\n",
        "            'per_device_train_batch_size': 4\n",
        "        }\n",
        "        batch_size = hyperparameters['per_device_train_batch_size']\n",
        "        logging_steps = len(self.treino) // batch_size\n",
        "        epochs = hyperparameters['num_train_epochs']\n",
        "        training_args = TrainingArguments(\n",
        "            output_dir = \"results\",\n",
        "            num_train_epochs = epochs,\n",
        "            per_device_train_batch_size = batch_size,\n",
        "            per_device_eval_batch_size = batch_size,\n",
        "            evaluation_strategy = \"epoch\",\n",
        "            metric_for_best_model = \"f1\",\n",
        "            disable_tqdm = False,\n",
        "            logging_steps = logging_steps,\n",
        "            gradient_accumulation_steps = 2,\n",
        "            eval_accumulation_steps = 2,\n",
        "            learning_rate = hyperparameters['learning_rate'],\n",
        "        )\n",
        "        trainer = Trainer(\n",
        "            model_init=model_init,\n",
        "            args=training_args,\n",
        "            train_dataset=self.treino,\n",
        "            eval_dataset=self.teste,\n",
        "            data_collator=data_collator,\n",
        "            tokenizer=self.tokenizer,\n",
        "            compute_metrics=compute_metrics\n",
        "        )\n",
        "\n",
        "        return trainer\n",
        "\n",
        "\n",
        "    def __set_wandb_run_name(self, run_name: str):\n",
        "        if run_name is None:\n",
        "            run_name = \"huggingface\"\n",
        "\n",
        "        setattr(self.trainer.args, \"run_name\", run_name)"
      ],
      "metadata": {
        "id": "TMtqVe9__Ojb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pipeline\n",
        "def get_trainer(\n",
        "                dataset: dict,\n",
        "                label_names,\n",
        "                metric,\n",
        "                balance=True,\n",
        "                stride=0,\n",
        "                tokenizer=None,\n",
        "                test_size=0.2,\n",
        "                random_state=42,\n",
        "                balancing_upper_limit=0.75,\n",
        "                balancing_range=0.20,\n",
        "                entities_names=None,\n",
        "                use_wandb = False,\n",
        "                wandb_run_name = None,\n",
        "                ):\n",
        "    #dataset\n",
        "    treino, teste = train_test_split(dataset,\n",
        "                                    test_size=test_size,\n",
        "                                    random_state=random_state)\n",
        "\n",
        "    #balanceamento\n",
        "    if balance:\n",
        "        balance_datasets(treino, teste,\n",
        "                        upper_limit=balancing_upper_limit,\n",
        "                        balancing_range=balancing_range,\n",
        "                        names_list=entities_names)\n",
        "\n",
        "    #tokenizaÃ§Ã£o\n",
        "    if not tokenizer:\n",
        "        tokenizer = AutoTokenizer.from_pretrained('neuralmind/bert-base-portuguese-cased',\n",
        "                                                    do_lower_case=False)\n",
        "    treino = tokenize_dataset(treino, tokenizer,\n",
        "                                stride=stride)\n",
        "    teste = tokenize_dataset(teste, tokenizer,\n",
        "                                stride=stride)\n",
        "\n",
        "    trainer = NM_Trainer(treino, teste,\n",
        "                        label_names=label_names,\n",
        "                        metric=metric,\n",
        "                        tokenizer=tokenizer,\n",
        "                        use_wandb=use_wandb,\n",
        "                        wandb_run_name=wandb_run_name,)\n",
        "\n",
        "    return trainer\n",
        "\n",
        "\n",
        "def run_pbt(\n",
        "        pbt_scheduler,\n",
        "        dataset: dict,\n",
        "        label_names,\n",
        "        metric,\n",
        "        balance=True,\n",
        "        stride=0,\n",
        "        tokenizer=None,\n",
        "        test_size=0.2,\n",
        "        random_state=42,\n",
        "        balancing_upper_limit=0.75,\n",
        "        balancing_range=0.20,\n",
        "        entities_names=None,\n",
        "        use_wandb = False,\n",
        "        wandb_config=None,\n",
        "        ):\n",
        "    trainer = get_trainer(dataset=dataset,\n",
        "                        label_names=label_names,\n",
        "                        metric=metric,\n",
        "                        balance=balance,\n",
        "                        stride=stride,\n",
        "                        tokenizer=tokenizer,\n",
        "                        test_size=test_size,\n",
        "                        random_state=random_state,\n",
        "                        balancing_upper_limit=balancing_upper_limit,\n",
        "                        balancing_range=balancing_range,\n",
        "                        entities_names=entities_names,)\n",
        "\n",
        "    if use_wandb:\n",
        "        wandb.init(reinit=True,config=wandb_config)\n",
        "\n",
        "    best_trial = trainer.hyperparameter_search(\n",
        "        direction=\"maximize\",\n",
        "        backend=\"ray\",\n",
        "        scheduler=pbt_scheduler,\n",
        "    )\n",
        "\n",
        "    return best_trial"
      ],
      "metadata": {
        "id": "oZd_ap9F_aha"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset and globals"
      ],
      "metadata": {
        "id": "r4LsE4Pq_pdL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zZk_JMfAXHWj"
      },
      "outputs": [],
      "source": [
        "!gdown \"1XYdcOxnr-esES8bwKezTb6MYMfhQQriW\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# globals\n",
        "data_path = \"NM_dataset.json\"\n",
        "dataset = [doc for doc in json2dict(data_path)]\n",
        "tokenizer = AutoTokenizer.from_pretrained('neuralmind/bert-base-portuguese-cased', do_lower_case=False)\n",
        "metric = load_metric(\"seqeval\")\n",
        "entities_names = ['COMECO RECORTE', 'CABECALHO', 'SUBCABECALHO']\n",
        "label_names={\n",
        "    0: 'O',\n",
        "    1: 'B-CABECALHO',\n",
        "    2: 'I-CABECALHO',\n",
        "    3: 'B-SUBCABECALHO',\n",
        "    4: 'I-SUBCABECALHO',\n",
        "    5: 'B-COMECO_RECORTE',\n",
        "    6: 'I-COMECO_RECORTE',\n",
        "}"
      ],
      "metadata": {
        "id": "koDOZ05m_4W5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def my_objective(metrics):\n",
        "    return metrics[\"eval_f1\"]\n",
        "\n",
        "pbt_scheduler = PopulationBasedTraining(\n",
        "    time_attr='training_iteration',\n",
        "    metric=my_objective,\n",
        "    mode='max',\n",
        "    perturbation_interval=600.0,\n",
        "    hyperparam_mutations={\n",
        "        \"learning_rate\": tune.loguniform(6e-6, 1e-3),\n",
        "        \"num_train_epochs\": tune.choice(range(5, 15)),\n",
        "        # \"seed\": tune.choice(range(1, 41)),\n",
        "        \"per_device_train_batch_size\": tune.choice([4, 8, 16]),\n",
        "    })\n",
        "\n",
        "wandb_config = {\n",
        "    \"project\": \"PBT_Optimization_Project\",\n",
        "    \"entity\": \"chinagab\",\n",
        "    \"api_key\": \"7d7deda5ab99137996e34e47dc688b1d6b4d179c\",\n",
        "    \"log_config\": True\n",
        "}\n",
        "\n",
        "best_trial = run_pbt(pbt_scheduler=pbt_scheduler,\n",
        "                     dataset=dataset,\n",
        "                     label_names=label_names,\n",
        "                     metric=metric,\n",
        "                     balance=True,\n",
        "                     stride=0,\n",
        "                     tokenizer=tokenizer,\n",
        "                     test_size=0.2,\n",
        "                     random_state=42,\n",
        "                     balancing_upper_limit=0.75,\n",
        "                     balancing_range=0.20,\n",
        "                     entities_names=entities_names,\n",
        "                     use_wandb = True,\n",
        "                     wandb_config=wandb_config,)"
      ],
      "metadata": {
        "id": "hluQ4V8nFQbP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "938cCSFtr7qQ"
      },
      "outputs": [],
      "source": [
        "trainer.args"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Psi4JymeIrJs"
      },
      "outputs": [],
      "source": [
        "best_trial"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hyperparameters={'learning_rate': 4.076831342095183e-05, \n",
        "                 'num_train_epochs': 12, \n",
        "                 'per_device_train_batch_size': 4}"
      ],
      "metadata": {
        "id": "QwGtLALZCmjI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for n, v in hyperparameters.items():\n",
        "    print (n)\n",
        "    setattr(trainer.args, n, v)\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "05FNx8tgC2_1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EnbV685hI6G4"
      },
      "outputs": [],
      "source": [
        "predictions, labels, _ = trainer.predict(teste)\n",
        "\n",
        "predictions = np.argmax(predictions, axis=2)\n",
        "\n",
        "# Remove ignored index (special tokens)\n",
        "\n",
        "true_predictions = [\n",
        "\n",
        "[label_names[p] for (p, l) in zip(prediction, label) if l != -100]\n",
        "\n",
        "for prediction, label in zip(predictions, labels)\n",
        "\n",
        "]\n",
        "\n",
        "true_labels = [\n",
        "\n",
        "[label_names[l] for (p, l) in zip(prediction, label) if l != -100]\n",
        "\n",
        "for prediction, label in zip(predictions, labels)\n",
        "\n",
        "]\n",
        "print(type(true_predictions))\n",
        "results = metric.compute(predictions=true_predictions, references=true_labels)\n",
        "\n",
        "results"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}